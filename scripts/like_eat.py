from pathlib import Path
from multi_source_ad_etl.multi_source_ad_etl import MultiSourceAdETL
from google_cloud_client.google_cloud_client import GoogleCloudClient as gcc
import utils.utils as ut
import logging
import polars as pl
import multi_source_ad_etl.data_clean_lib as cln

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s %(name)s %(levelname)s: %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

data_dir = Path(__file__).parent.parent / "data"
like_eat_raw_dir = data_dir / "raw" / "like_eat"
processed_dir = data_dir / "proc"

# Create folders if they don't exist
data_dir.mkdir(parents=True, exist_ok=True)
like_eat_raw_dir.mkdir(parents=True, exist_ok=True)
processed_dir.mkdir(parents=True, exist_ok=True)

like_eat_mapping = {
    "Meta_naver": {
        "ì¼": "ì¼",
        "ìº í˜ì¸ ì´ë¦„": "ìº í˜ì¸ ì´ë¦„",
        "ê´‘ê³  ì„¸íŠ¸ ì´ë¦„": "ê´‘ê³  ì„¸íŠ¸ ì´ë¦„",
        "ê´‘ê³  ì´ë¦„": "ê´‘ê³  ì´ë¦„",
        "ì„±": "ì„±",
        "ì—°ë ¹": "ì—°ë ¹",
        "ì›¹ì‚¬ì´íŠ¸ URL": "ì›¹ì‚¬ì´íŠ¸ URL",
        "ì§€ì¶œ ê¸ˆì•¡ (KRW)": "ì§€ì¶œ ê¸ˆì•¡ (KRW)",
        "ë…¸ì¶œ": "ë…¸ì¶œ",
        "ë¹ˆë„": "ë¹ˆë„",
        "ë„ë‹¬": "ë„ë‹¬",
        "ë§í¬ í´ë¦­": "ë§í¬ í´ë¦­",
        "ê³µìœ  í•­ëª©ì´ í¬í•¨ëœ ì¥ë°”êµ¬ë‹ˆì— ë‹´ê¸°": "ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°",
        "ê³µìœ  í•­ëª©ì´ í¬í•¨ëœ êµ¬ë§¤": "êµ¬ë§¤",
        "ê³µìœ  í•­ëª©ì˜ êµ¬ë§¤ ì „í™˜ê°’": "êµ¬ë§¤ ì „í™˜ê°’",
        "ë™ì˜ìƒ 25% ì¬ìƒ": "ë™ì˜ìƒ 25% ì¬ìƒ",
        "ë™ì˜ìƒ 50% ì¬ìƒ": "ë™ì˜ìƒ 50% ì¬ìƒ",
        "ë™ì˜ìƒ 75% ì¬ìƒ": "ë™ì˜ìƒ 75% ì¬ìƒ",
        "ë™ì˜ìƒ 95% ì¬ìƒ": "ë™ì˜ìƒ 95% ì¬ìƒ",
        "ë™ì˜ìƒ 100% ì¬ìƒ": "ë™ì˜ìƒ 100% ì¬ìƒ",
        "ë™ì˜ìƒ ì¬ìƒ": "ë™ì˜ìƒ ì¬ìƒ",
        "ThruPlay": "ThruPlay",
    },
    "Naver_GFA": {
        "ê¸°ê°„": "ì¼",
        "ì• ì…‹ ê·¸ë£¹ ì´ë¦„": "ê´‘ê³  ì„¸íŠ¸ ì´ë¦„",
        "ìº í˜ì¸ ì´ë¦„": "ìº í˜ì¸ ì´ë¦„",
        "ì´ ë¹„ìš©": "ì§€ì¶œ ê¸ˆì•¡ (KRW)",
        "ë…¸ì¶œ": "ë…¸ì¶œ",
        "í´ë¦­": "ë§í¬ í´ë¦­",
        "êµ¬ë§¤ì™„ë£Œìˆ˜": "êµ¬ë§¤",
        "ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°ìˆ˜": "ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°",
        "êµ¬ë§¤ì™„ë£Œ ì „í™˜ ë§¤ì¶œì•¡": "êµ¬ë§¤ ì „í™˜ê°’",
    },
}

# pl.Int64ëŠ” ì •ìˆ˜, Pl.Stringì€ ë¬¸ìíƒ€ì…, Pl.Float64ëŠ” ì†Œìˆ˜, Pl.dateëŠ” ë‚ ì§œë¡œ ìœ„ ë§µí•‘í•œ ìš°ì¸¡ ë‹¨ì–´ë¥¼ í™œìš©í•˜ì—¬ ì¶”ê°€í•˜ë©´ ë¨
like_eat_standard_schema = {
    "Source": pl.String,
    "ì¼": pl.Date,
    "ìº í˜ì¸ ì´ë¦„": pl.String,
    "ê´‘ê³  ì„¸íŠ¸ ì´ë¦„": pl.String,
    "ê´‘ê³  ì´ë¦„": pl.String,
    "ì„±": pl.String,
    "ì—°ë ¹": pl.String,
    "ì›¹ì‚¬ì´íŠ¸ URL": pl.String,
    "ì§€ì¶œ ê¸ˆì•¡ (KRW)": pl.Float64,
    "ë…¸ì¶œ": pl.Int64,
    "ë¹ˆë„": pl.Float64,
    "ë„ë‹¬": pl.Int64,
    "ë§í¬ í´ë¦­": pl.Int64,
    "ì¥ë°”êµ¬ë‹ˆ ë‹´ê¸°": pl.Int64,
    "êµ¬ë§¤": pl.Int64,
    "êµ¬ë§¤ ì „í™˜ê°’": pl.Float64,
    "ë™ì˜ìƒ 25% ì¬ìƒ": pl.Int64,
    "ë™ì˜ìƒ 50% ì¬ìƒ": pl.Int64,
    "ë™ì˜ìƒ 75% ì¬ìƒ": pl.Int64,
    "ë™ì˜ìƒ 95% ì¬ìƒ": pl.Int64,
    "ë™ì˜ìƒ 100% ì¬ìƒ": pl.Int64,
    "ë™ì˜ìƒ ì¬ìƒ": pl.Int64,
    "ThruPlay": pl.Int64,
}

like_eat_src_criteria = {
    "Meta_naver": {"ê³µìœ  í•­ëª©ì´ í¬í•¨ëœ êµ¬ë§¤", "ê³µìœ  í•­ëª©ì´ í¬í•¨ëœ ì¥ë°”êµ¬ë‹ˆì— ë‹´ê¸°"},
    "Naver_GFA": {
        "ì—°ë ¹ ë° ì„±ë³„",
        "ì• ì…‹ ê·¸ë£¹ ì´ë¦„",
    },
}

cleaners = {
    "Naver_GFA": [cln.clean_naver_gfa_age_gender, cln.clean_naver_gfa_date],
}

like_eat = MultiSourceAdETL(
    raw_dir=like_eat_raw_dir,
    source_criteria=like_eat_src_criteria,
    rename_mappings=like_eat_mapping,
    standard_schema=like_eat_standard_schema,
    cleaning_functions=cleaners,
)

like_eat_merged = (
    like_eat.read_tabular_files()
    .capitalize_col_names()
    .assign_source()
    .clean_dataframes()
    .standardize_dataframes()
    .merge_and_collect()
)

like_eat_out = processed_dir / ut.make_date_filename("like_eat", like_eat_merged)

daily_exports = {
    "like_eat": {
        "export": True,  # True means that export to the proc dir
        "upload": True,  # True means that upload to the sheet
        "df": like_eat_merged,
        "sheet_key": "1qS-g-grvB1VyzVv3NUgVzEMSM8VWJOKD0_ceC3RyTsI",
        "sheet_name": "raw",  # ğŸ‘‹ Don't forget to change this part!!!!!
        "out": like_eat_out,
    },
}

gcloud_credential = Path(__file__).parent.parent / "gcloud_credential.json"
gs = gcc(gcloud_credential).googlesheet

for name, config in daily_exports.items():
    export_df: pl.DataFrame = config["df"]
    if config["upload"]:
        # Clear range and notice how the `range_mode = "column_range"`
        gs.clear_range(
            sheet_key=config["sheet_key"],
            sheet_name=config["sheet_name"],
            range=ut.df_to_a1(export_df, range_mode="column_range"),
        )

        # Upload df and notice how the `range_mode = "full_range"`
        gs.upload_dataframe(
            df=export_df,
            sheet_key=config["sheet_key"],
            sheet_name=config["sheet_name"],
            range=ut.df_to_a1(export_df, range_mode="full_range"),
        )

    if config["export"]:
        # Export csv
        export_df.write_csv(config["out"], include_bom=True)
        logging.info(f"File exported to {config['out']}")
